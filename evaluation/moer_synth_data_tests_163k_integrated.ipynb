{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(f\"/home/{os.getlogin()}/watttime-python-client-aer-algo\")\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pytz\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import concurrent.futures\n",
    "\n",
    "from watttime import WattTimeForecast, WattTimeHistorical\n",
    "\n",
    "import optimizer.s3 as s3u\n",
    "import evaluation.eval_framework as efu\n",
    "\n",
    "username = os.getenv(\"WATTTIME_USER\")\n",
    "password = os.getenv(\"WATTTIME_PASSWORD\")\n",
    "\n",
    "actual_data = WattTimeHistorical(username, password)\n",
    "hist_data = WattTimeForecast(username, password)\n",
    "\n",
    "s3 = s3u.s3_utils()\n",
    "key = '20240726_1k_synth_users_163_days.csv'\n",
    "generated_data = s3.load_csvdataframe(file=key)\n",
    "generated_data = generated_data[-8000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions = [\n",
    "# 'CAISO_NORTH',\n",
    "# 'SPP_TX',\n",
    "# 'ERCOT_EASTTX',\n",
    "# 'FPL',\n",
    "# 'SOCO',\n",
    "# 'PJM_CHICAGO',\n",
    "# 'LDWP',\n",
    "# 'PJM_DC',\n",
    "# 'NYISO_NYC'\n",
    "# ]\n",
    "regions = [\n",
    "    'PJM_CHICAGO',\n",
    "]\n",
    "region = regions[0]\n",
    "\n",
    "synth_data = generated_data.copy(deep=True)\n",
    "synth_data[\"plug_in_time\"] = pd.to_datetime(synth_data[\"plug_in_time\"])\n",
    "synth_data[\"unplug_time\"] = pd.to_datetime(synth_data[\"unplug_time\"])\n",
    "\n",
    "import pickle\n",
    "actual_pickle = s3.load_file(file=\"pjm_actual.pkl\")\n",
    "HISTORICAL_ACTUAL_CACHE = pickle.loads(actual_pickle)\n",
    "\n",
    "forecast_pickle = s3.load_file(file=\"pjm_forecast.pkl\")\n",
    "HISTORICAL_FORECAST_CACHE = pickle.loads(actual_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cached version of the get_*_data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def precache_actual_data(synth_data, regions):\n",
    "    distinct_dates = [\n",
    "        datetime.strptime(date, \"%Y-%m-%d\").date()\n",
    "        for date in synth_data[\"distinct_dates\"].unique().tolist()\n",
    "    ]\n",
    "    all_dates_regions = [(date, region) for date in distinct_dates for region in regions]\n",
    "\n",
    "    def get_actual_data_for_region_date(date, region):\n",
    "        start = pd.to_datetime(date)\n",
    "        end = start + pd.Timedelta(\"2d\")  \n",
    "        return (region, date, actual_data.get_historical_pandas(\n",
    "            start - pd.Timedelta(\"9h\"),\n",
    "            end + pd.Timedelta(\"9h\"),\n",
    "            region,\n",
    "        ))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        result = executor.map(get_actual_data_for_region_date,\n",
    "            [date for (date, region) in all_dates_regions], \n",
    "            [region for (date, region) in all_dates_regions]\n",
    "            )\n",
    "    result = list(result)\n",
    "\n",
    "    return {(region, date): data for (region, date, data) in result}\n",
    "\n",
    "HISTORICAL_ACTUAL_CACHE = precache_actual_data(synth_data, regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def precache_fcst_data(synth_data, regions):\n",
    "    distinct_dates = [\n",
    "        datetime.strptime(date, \"%Y-%m-%d\").date()\n",
    "        for date in synth_data[\"distinct_dates\"].unique().tolist()\n",
    "    ]\n",
    "    all_dates_regions = [(date, region) for date in distinct_dates for region in regions]\n",
    "\n",
    "    def get_fsct_data_for_region_date(date, region):\n",
    "        start = pd.to_datetime(date)\n",
    "        end = (pd.to_datetime(date) + pd.Timedelta(\"1d\"))      \n",
    "        return  (region, date,  hist_data.get_historical_forecast_pandas(\n",
    "            start - pd.Timedelta(\"9h\"),\n",
    "            end + pd.Timedelta(\"9h\"),\n",
    "            region,\n",
    "        ))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        result = executor.map(get_fsct_data_for_region_date,\n",
    "            [date for (date, region) in all_dates_regions], \n",
    "            [region for (date, region) in all_dates_regions]\n",
    "            )\n",
    "    result = list(result)\n",
    "    return {(region, date): data for (region, date, data) in result}\n",
    "\n",
    "HISTORICAL_FORECAST_CACHE = precache_fcst_data(synth_data, regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_fcst_data_cached(plug_in_time, horizon, region):\n",
    "    time_zone = efu.get_timezone_from_dict(region)\n",
    "    plug_in_time_utc = pd.Timestamp(efu.convert_to_utc(plug_in_time, time_zone))\n",
    "    date = plug_in_time.date()\n",
    "    if (region, date) not in HISTORICAL_FORECAST_CACHE.keys():\n",
    "        print(type(date), date)\n",
    "        start = pd.to_datetime(date)\n",
    "        end = (pd.to_datetime(date) + pd.Timedelta(\"1d\"))      \n",
    "        HISTORICAL_FORECAST_CACHE[(region, date)] = hist_data.get_historical_forecast_pandas(\n",
    "            start - pd.Timedelta(\"9h\"),\n",
    "            end + pd.Timedelta(\"9h\"),\n",
    "            region,\n",
    "        )\n",
    "    cache = HISTORICAL_FORECAST_CACHE[(region, date)]\n",
    "        \n",
    "    # make this match efu.get_historical_fsct_data\n",
    "    generated_at_times = cache[\"generated_at\"].unique()\n",
    "    generated_at = max([t for t in generated_at_times if t < plug_in_time_utc])\n",
    "    df = cache[cache[\"generated_at\"] == generated_at].copy()\n",
    "    return df.iloc[:math.ceil(horizon / 12) * 12]\n",
    "\n",
    "def get_historical_actual_data_cached(plug_in_time, horizon, region):\n",
    "    time_zone = efu.get_timezone_from_dict(region)\n",
    "    plug_in_time_utc = pd.Timestamp(efu.convert_to_utc(plug_in_time, time_zone))\n",
    "    date = plug_in_time.date()\n",
    "    \n",
    "    if (region, date) not in HISTORICAL_ACTUAL_CACHE.keys():\n",
    "        start = pd.to_datetime(date)\n",
    "        end = (pd.to_datetime(date) + pd.Timedelta(\"2d\"))   \n",
    "        HISTORICAL_ACTUAL_CACHE[(region, date)] = actual_data.get_historical_pandas(\n",
    "            start - pd.Timedelta(\"9h\"),\n",
    "            end + pd.Timedelta(\"9h\"),\n",
    "            region,\n",
    "        )\n",
    "    cache = HISTORICAL_ACTUAL_CACHE[(region, date)]\n",
    "\n",
    "    t_start = max([t for t in cache[\"point_time\"].unique() if t < plug_in_time_utc])\n",
    "    df =  cache[cache[\"point_time\"] >= t_start].copy()\n",
    "    return df.iloc[:math.ceil(horizon / 12) * 12 + 1].reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Data with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "synth_data['moer_data'] = synth_data.apply(\n",
    "    lambda x: get_historical_fcst_data_cached(\n",
    "    x.plug_in_time,\n",
    "    math.ceil(x.total_intervals_plugged_in),\n",
    "    region = region\n",
    "    ), axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "synth_data['moer_data_actual'] = synth_data.apply(\n",
    "    lambda x: get_historical_actual_data_cached(\n",
    "    x.plug_in_time,\n",
    "    math.ceil(x.total_intervals_plugged_in),\n",
    "    region = region\n",
    "    ), axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOER - No Optimization - Actual Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "get_charging_schedule_lambda = lambda x: x[\"usage\"].values.flatten()\n",
    "get_total_emissions_lambda = lambda  x: x[\"emissions_co2e_lb\"].sum()\n",
    "\n",
    "synth_data['charger_baseline_actual_api'] = synth_data.apply(\n",
    "    lambda x: efu.get_schedule_and_cost_v2(\n",
    "        x.power_output_rate,\n",
    "        math.ceil(min(x.total_seconds_to_95, x.length_plugged_in) / 300.0) * 5.0,\n",
    "        x.moer_data_actual,\n",
    "        optimization_method='baseline'\n",
    "        ), \n",
    "        axis = 1\n",
    "        )\n",
    "\n",
    "synth_data['baseline_charging_schedule_api'] = synth_data['charger_baseline_actual_api'].apply(get_charging_schedule_lambda)\n",
    "synth_data['baseline_actual_emissions_api'] = synth_data['charger_baseline_actual_api'].apply(get_total_emissions_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOER - Simple Optimization - Forecast Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: I feel like this slicing might lead to bugs in the future\n",
    "def get_total_emission(moer, schedule):\n",
    "    x = np.array(schedule).flatten()\n",
    "    return np.dot(moer[:x.shape[0]], x)\n",
    "\n",
    "synth_data['charger_simple_forecast'] = synth_data.apply(\n",
    "    lambda x: efu.get_schedule_and_cost_v2(\n",
    "        x.power_output_rate,\n",
    "        int(math.ceil(min(x.total_seconds_to_95, x.length_plugged_in) / 300.0) * 5),\n",
    "        x.moer_data,\n",
    "        optimization_method='simple'\n",
    "        ), \n",
    "        axis = 1\n",
    "        )\n",
    "\n",
    "synth_data['simple_charging_schedule'] = synth_data['charger_simple_forecast'].apply(get_charging_schedule_lambda)\n",
    "synth_data['simple_estimated_emissions'] = synth_data['charger_simple_forecast'].apply(get_total_emissions_lambda)\n",
    "synth_data[\"simple_actual_emissions\"] = synth_data.apply(\n",
    "    lambda x: get_total_emission(\n",
    "        x.moer_data_actual['value'],\n",
    "        x.charger_simple_forecast.energy_usage_mwh,\n",
    "    ),\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "synth_data['charger_simple_actual']= synth_data.apply(\n",
    "    lambda x: efu.get_schedule_and_cost_v2(\n",
    "        x.power_output_rate,\n",
    "        int(math.ceil(min(x.total_seconds_to_95, x.length_plugged_in) / 300.0) * 5),\n",
    "        x.moer_data_actual,\n",
    "        optimization_method='simple'\n",
    "        ), \n",
    "        axis = 1\n",
    "        )\n",
    "\n",
    "\n",
    "synth_data['simple_actual_charging_schedule'] = synth_data['charger_simple_actual'].apply(get_charging_schedule_lambda)\n",
    "synth_data['simple_ideal_emissions'] = synth_data['charger_simple_actual'].apply(get_total_emissions_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer.s3 import s3_utils\n",
    "s3 = s3_utils()\n",
    "cols = [\n",
    "    \"user_type\",\n",
    "    \"power_output_rate\",\n",
    "    \"distinct_dates\",\n",
    "    \"plug_in_time\",\n",
    "    \"total_intervals_plugged_in\",\n",
    "    \"charged_kWh_actual\",\n",
    "    \"MWh_fraction\",\n",
    "    \"simple_actual_emissions\",\n",
    "    \"baseline_actual_emissions\",\n",
    "    \"simple_estimated_emissions\",\n",
    "    \"simple_ideal_emissions\"\n",
    "]\n",
    "\n",
    "s3.store_csvdataframe(\n",
    "    synth_data[cols], f\"results_v2/20240726_1k_synth_users_163_days_{region}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
